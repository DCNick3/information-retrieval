{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evolving vector-space model\n",
    "This part will be devoted to the use of ML model for the needs of information retrieval and text classification.  \n",
    "\n",
    "### **Searching in the curious facts database**\n",
    "\n",
    "The facts dataset is given [here](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), take a look. We want you to retrieve facts **relevant to the query** (whatever it means), for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using pretrained ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Use neural networks to embed sentences\n",
    "\n",
    "Make use of any, starting from doc2vec up to Transformers, etc.\n",
    "\n",
    "\n",
    "- [UCE in spacy 2](https://spacy.io/universe/project/spacy-universal-sentence-encoder) (`!pip install spacy-universal-sentence-encoder`)\n",
    "- [Sentence BERT in spacy 2](https://spacy.io/universe/project/spacy-sentence-bert) (`!pip install spacy-sentence-bert`)\n",
    "- [Pretrained ðŸ¤— Transformers](https://huggingface.co/transformers/pretrained_models.html)\n",
    "- [Spacy 3 transformers](https://spacy.io/usage/embeddings-transformers#transformers-installation)\n",
    "- [doc2vec pretrained](https://github.com/jhlau/doc2vec)\n",
    "- [Some more sentence transformers](https://www.sbert.net/docs/quickstart.html) <-- the simplest option.\n",
    "\n",
    "Do dependency installation first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then they the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Write the function, that prepares sentence embedding\n",
    "\n",
    "Implement the function, which returns a fixed-sized vector given a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text):\n",
    "    #TODO return a fixed-sized vector of embedding\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(\n",
    "            embed(\"Some random text\")\n",
    "        ) == len(\n",
    "            embed(\"Folks, here's a story about Minnie the Moocher. She was a lowdown hoochie coocher. She was the roughest, toughest frail, but Minnie had a heart as big as a whale\")\n",
    "        ), \"Length should match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "assert abs(cosine(\n",
    "            embed(\"some text for testing\"), \n",
    "            embed(\"some text for testing\")\n",
    "        )) < 1e-4, \"Embedding should be equal\"\n",
    "\n",
    "assert abs(cosine(\n",
    "            embed(\"Cats eat mice.\"), \n",
    "            embed(\"Terminator is an autonomous cyborg, typically humanoid, originally conceived as a virtually indestructible soldier, infiltrator, and assassin.\")\n",
    "        )) > 0.2, \"Embeddings should be far\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Reading the data\n",
    "\n",
    "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "\n",
    "#TODO read facts into a list of facts\n",
    "facts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*facts[:5], sep='\\n')\n",
    "\n",
    "assert len(facts) == 159\n",
    "assert ('our lovely little planet') in facts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Transforming sentences to vectors\n",
    "\n",
    "Transform the list of facts to `numpy.array` of vectors corresponding to each document (`sent_vecs`), inferring them from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO infer vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent_vecs.shape[0] == len(facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Find closest\n",
    "\n",
    "Now find 5 facts which are closest to the query using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: good mood\n",
      "\t 67. The chance of you dying on the way to get lottery tickets is actually greater than your chance of winning.\n",
      "\t 84. You are 1% shorter in the evening than in the morning\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
      "\t 10. If you believe that you're truly one in a million, there are still approximately 7,184 more people out there just like you.\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n"
     ]
    }
   ],
   "source": [
    "#TODO output closest facts to the query\n",
    "query = \"good mood\"\n",
    "\n",
    "print(\"Results for query:\", query)\n",
    "for k in #...\n",
    "    print(\"\\t\", facts[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Recommend 5 facts to each of the queries for the following query bucket\n",
    "```\n",
    "good mood\n",
    "gorilla\n",
    "woman\n",
    "earth\n",
    "japan\n",
    "people\n",
    "math\n",
    "```\n",
    "\n",
    "Recommend 5 facts to each of the queries. Write your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good mood\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n",
      "\t 10. If you believe that you're truly one in a million, there are still approximately 7,184 more people out there just like you.\n",
      "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 84. You are 1% shorter in the evening than in the morning\n",
      "gorilla\n",
      "\t 106. The male ostrich can roar just like a lion.\n",
      "\t 85. The elephant is the only mammal that can't jump!\n",
      "\t 107. Mountain lions can whistle.\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 139. Beetles taste like apples, wasps like pine nuts, and worms like fried bacon.\n",
      "woman\n",
      "\t 131. If a pregnant woman has organ damage, the baby in her womb sends stem cells to help repair the organ.\n",
      "\t 65. A Swedish woman lost her wedding ring, and found it 16 years later- growing on a carrot in her garden.\n",
      "\t 106. The male ostrich can roar just like a lion.\n",
      "\t 88. Earth is the only planet that is not named after a god.\n",
      "\t 146. In France, it is legal to marry a dead person.\n",
      "earth\n",
      "\t 88. Earth is the only planet that is not named after a god.\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\t 126. Saturn's density is low enough that the planet would float in water.\n",
      "\t 153. For every human on Earth there are 1.6 million ants.\n",
      "\t 155. On Jupiter and Saturn it rains diamonds.\n",
      "japan\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n",
      "\t 17. Coca-Cola would be green if coloring wasn't added to it.\n",
      "\t 66. Donald duck comics were banned from Finland because he doesn't wear pants.\n",
      "\t 77. More than 60,000 people are flying over the United States in an airplane right now.\n",
      "\t 64. In Japan, crooked teeth are considered cute and attractive.\n",
      "people\n",
      "\t 34. 95% of people text things they could never say in person.\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\t 109. Cows kill more people than sharks do.\n",
      "\t 10. If you believe that you're truly one in a million, there are still approximately 7,184 more people out there just like you.\n",
      "\t 87. If 33 million people held hands, they could make it all the way around the equator.\n",
      "math\n",
      "\t 97. 111,111,111 X 111,111,111 = 12,345,678,987,654,321\n",
      "\t 119. Dogs are capable of understanding up to 250 words and gestures and have demonstrated the ability to do simple mathematical calculations.\n",
      "\t 5. You burn more calories sleeping than you do watching television.\n",
      "\t 48. Chewing gum burns about 11 calories per hour.\n",
      "\t 34. 95% of people text things they could never say in person.\n"
     ]
    }
   ],
   "source": [
    "# write your code or computations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. [BONUS] Write your own relevance assessments and compute DCG@5\n",
    "Compare you results accross the group. Which embedding model performs the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add assesment\n",
    "assessment = []\n",
    "\n",
    "def dcg(rels):\n",
    "    #TODO compute DCG@5\n",
    "    return res\n",
    "\n",
    "sum([dcg(row) for row in assessment]) / len(assessment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
